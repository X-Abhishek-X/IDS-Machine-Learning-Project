{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intrusion Detection System (IDS) - Local Version\n",
        "This notebook implements an IDS using NSL-KDD dataset with ML and Neural Network models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import socket\n",
        "import threading\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Preprocess Data\n",
        "Loading NSL-KDD dataset from local files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets (NSL-KDD format: 41 features + label)\n",
        "columns = [f'feature_{i}' for i in range(41)] + ['label']\n",
        "\n",
        "# Load from local directory\n",
        "train_data = pd.read_csv('KDDTrain+.txt', header=None, names=columns)\n",
        "test_data = pd.read_csv('KDDTest+.txt', header=None, names=columns)\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Testing data shape: {test_data.shape}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(train_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode categorical features (protocol_type, service, flag)\n",
        "# These are typically in columns 1, 2, 3\n",
        "categorical_cols = ['feature_1', 'feature_2', 'feature_3']\n",
        "\n",
        "le_cat = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    train_data[col] = le_cat.fit_transform(train_data[col].astype(str))\n",
        "    test_data[col] = le_cat.transform(test_data[col].astype(str))\n",
        "\n",
        "# Encode labels (normal=0, attack=1)\n",
        "le_label = LabelEncoder()\n",
        "train_data['label'] = le_label.fit_transform(train_data['label'])\n",
        "test_data['label'] = le_label.transform(test_data['label'])\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = train_data.drop('label', axis=1)\n",
        "y_train = train_data['label']\n",
        "X_test = test_data.drop('label', axis=1)\n",
        "y_test = test_data['label']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Datasets loaded and preprocessed.\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Part 1: EDA ---\")\n",
        "print(f\"Training dataset shape: {X_train.shape}\")\n",
        "print(f\"Testing dataset shape: {X_test.shape}\")\n",
        "print(\"Summary statistics for training dataset:\")\n",
        "print(train_data.describe())\n",
        "\n",
        "# Percentage distribution of normal and attack records\n",
        "train_label_counts = y_train.value_counts()\n",
        "print(f\"\\nTraining: Normal: {train_label_counts.get(0, 0)/len(y_train)*100:.2f}%, Attack: {train_label_counts.get(1, 0)/len(y_train)*100:.2f}%\")\n",
        "\n",
        "# Bar chart for normal vs attack in train/test\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "train_label_counts.plot(kind='bar', ax=ax[0], title='Training Dataset')\n",
        "ax[0].set_xlabel('Label')\n",
        "ax[0].set_ylabel('Count')\n",
        "\n",
        "test_label_counts = y_test.value_counts()\n",
        "test_label_counts.plot(kind='bar', ax=ax[1], title='Testing Dataset')\n",
        "ax[1].set_xlabel('Label')\n",
        "ax[1].set_ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap (using a subset for visibility)\n",
        "corr = train_data.corr()\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap (Training Dataset)')\n",
        "plt.show()\n",
        "\n",
        "# Attack types distribution\n",
        "attack_types = train_data['label'].value_counts()\n",
        "attack_types.plot(kind='bar', title='Attack Types Distribution (Training)')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: ML Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Part 2: ML Models ---\")\n",
        "# Model 1: Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "rf_pred = rf_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"Random Forest Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, rf_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, rf_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, rf_pred, average='weighted'):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 2: SVM\n",
        "svm_model = SVC(kernel='rbf', random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "svm_pred = svm_model.predict(X_test_scaled)\n",
        "\n",
        "print(\"\\nSVM Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, svm_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, svm_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, svm_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, svm_pred, average='weighted'):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Neural Network Model (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Part 3: Neural Network ---\")\n",
        "\n",
        "class IDSNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(IDSNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 2)  # Binary classification\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for PyTorch\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "nn_model = IDSNet(X_train.shape[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = nn_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "nn_model.eval()\n",
        "with torch.no_grad():\n",
        "    nn_outputs = nn_model(X_test_tensor)\n",
        "    _, nn_pred = torch.max(nn_outputs, 1)\n",
        "    nn_pred = nn_pred.numpy()\n",
        "\n",
        "print(\"\\nNeural Network Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, nn_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, nn_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, nn_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, nn_pred, average='weighted'):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Model Comparison and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Part 4: Model Comparison ---\")\n",
        "models = ['Random Forest', 'SVM', 'Neural Network']\n",
        "accuracies = [accuracy_score(y_test, rf_pred), accuracy_score(y_test, svm_pred), accuracy_score(y_test, nn_pred)]\n",
        "precisions = [precision_score(y_test, rf_pred, average='weighted'), precision_score(y_test, svm_pred, average='weighted'), precision_score(y_test, nn_pred, average='weighted')]\n",
        "recalls = [recall_score(y_test, rf_pred, average='weighted'), recall_score(y_test, svm_pred, average='weighted'), recall_score(y_test, nn_pred, average='weighted')]\n",
        "f1s = [f1_score(y_test, rf_pred, average='weighted'), f1_score(y_test, svm_pred, average='weighted'), f1_score(y_test, nn_pred, average='weighted')]\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': models,\n",
        "    'Accuracy': accuracies,\n",
        "    'Precision': precisions,\n",
        "    'Recall': recalls,\n",
        "    'F1-Score': f1s\n",
        "})\n",
        "print(comparison_df)\n",
        "\n",
        "# Visualization\n",
        "comparison_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score']].plot(kind='bar', figsize=(10, 6))\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Real-Time IDS Prototype\n",
        "Note: The server/client code below is for demonstration. Run server and client in separate terminals for actual testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n--- Part 5: Real-Time IDS Prototype ---\")\n",
        "\n",
        "# Use Random Forest as the trained model for predictions\n",
        "def predict_traffic(data):\n",
        "    data_scaled = scaler.transform([data])\n",
        "    pred = rf_model.predict(data_scaled)[0]\n",
        "    return \"Normal\" if pred == 0 else \"Anomalous\"\n",
        "\n",
        "# Server function\n",
        "def server():\n",
        "    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    server_socket.bind(('localhost', 12345))\n",
        "    server_socket.listen(1)\n",
        "    print(\"Server listening on port 12345...\")\n",
        "    conn, addr = server_socket.accept()\n",
        "    print(f\"Connected to {addr}\")\n",
        "    while True:\n",
        "        data = conn.recv(1024)\n",
        "        if not data:\n",
        "            break\n",
        "        sample = pickle.loads(data)\n",
        "        result = predict_traffic(sample)\n",
        "        conn.send(result.encode())\n",
        "    conn.close()\n",
        "    server_socket.close()\n",
        "\n",
        "# Client function\n",
        "def client():\n",
        "    import time\n",
        "    time.sleep(1)  # Wait for server to start\n",
        "    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    client_socket.connect(('localhost', 12345))\n",
        "    sample_data = X_test.iloc[0].tolist()\n",
        "    client_socket.send(pickle.dumps(sample_data))\n",
        "    response = client_socket.recv(1024).decode()\n",
        "    print(f\"Prediction: {response}\")\n",
        "    client_socket.close()\n",
        "\n",
        "# Uncomment to run (note: may not work well in Jupyter, better in separate scripts)\n",
        "# threading.Thread(target=server).start()\n",
        "# client()\n",
        "\n",
        "print(\"\\nCoursework code execution complete.\")\n",
        "print(\"Prepare report and video based on outputs.\")"
      ]
    }
  ]
}
